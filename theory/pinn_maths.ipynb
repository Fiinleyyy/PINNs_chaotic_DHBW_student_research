{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb7da7f",
   "metadata": {},
   "source": [
    "## Mathematische Grundlagen eines PINN\n",
    "Nachfolgender Programmcode dient dazu die mathematischen Grundlagen eines Physic Informed Neural Networks zu veranschaulichen. \n",
    "Insbesondere soll klar werden, wie der Gradient der Kostenfunktion des PINN berechnet werden kann, auch ohne Zuhilfenahme der autograd-Funktion von Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74429031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3202b236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktion zur Berechnung der Sigmoid Funktion\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce4f3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Überprüfung der korrekten Implementierung der Sigmoid Hilfsfunktion\n",
    "x_0 = torch.tensor(0.0).view(-1, 1)\n",
    "s_0 = sigmoid(x_0)\n",
    "s_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b9ee20",
   "metadata": {},
   "source": [
    "### PINN ohne Aktivierungsfunktion\n",
    "In untenstehendem Programmcode wird ein PINN mit der Kostenfunktion $C = (\\frac{dy}{dt} - 1)^2$ implementiert. Zunächst wird der Ausgabewert des Neuronalen Netzes $y$ berechnet. Dieser wird anschließend mithilfe des Backpropagation Algorithmus nach dem Eingabewert des Neuronalen Netzes $t$ abgeleitet. Die so berechnete Ableitung wird, wie zuvor beschrieben, in die Kostenfunktion integriert. Abschließend wird der Gradient dieser Kostenfunktion, die die Ableitung $\\frac{dy}{dt}$ enthält, erneut mittels Backpropagation ermittelt. \n",
    "\n",
    "Es ist zu beachten, dass dieses Beispiel insofern vereinfacht ist, dass das Neuronale Netz ohne Aktivierungsfunktion realisiert ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27d0662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Belegung der Gewichte, Bias und Eingangswert des NN\n",
    "a_0 = torch.tensor(2.0, requires_grad=True).view(-1, 1)\n",
    "\n",
    "W_1 = torch.tensor([1., -1., 2.], requires_grad=True).view(-1, 1)\n",
    "W_2 = torch.tensor([\n",
    "    [1., 1., 0.],\n",
    "    [0., -1., 2.],\n",
    "    [0., -1., 1]\n",
    "])\n",
    "W_3 = torch.tensor([1., 0., 1.], requires_grad=True).view(1, -1)\n",
    "\n",
    "b_1 = torch.tensor([0., -1., 1.]).view(-1, 1)\n",
    "b_2 = torch.tensor([-1., -1., 2.]).view(-1, 1)\n",
    "b_3 = torch.tensor([-1.]).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a61f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung des Ausgangswertes des NN\n",
    "a_1 = torch.matmul(W_1, a_0) + b_1\n",
    "a_2 = torch.matmul(W_2, a_1) + b_2\n",
    "a_2.requires_grad_(True)\n",
    "a_3 = torch.matmul(W_3, a_2) + b_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655a8e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.]], grad_fn=<TBackward0>)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Berechung der Ableitung der Ausgabe des NN nach dem Eingabewert\n",
    "da3_a0 = torch.autograd.grad(a_3, a_0, create_graph=True)[0]\n",
    "da3_a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a4b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [0.],\n",
       "        [1.]], grad_fn=<TBackward0>)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manuelle Berechnung der Ableitung der Ausgabe nach der Eingabe\n",
    "c_0 = W_3.t()\n",
    "c_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0a2c7995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [0.],\n",
       "        [1.]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_1 = torch.matmul(W_2.t(), c_0)\n",
    "c_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "dade8a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_2 = torch.matmul(W_1.t(), c_1)\n",
    "c_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec39b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung der Kostenfunktion mithilfe der zuvor ermittelten Ableitung\n",
    "C = (da3_a0 - torch.tensor(1.0).view(-1, 1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8203075e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4.],\n",
       "         [0.],\n",
       "         [4.]]),)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Berechnung der partiellen Ableitung der Kostenfunktion nach den Gewichten W1\n",
    "dC_dw1 = torch.autograd.grad(C, W_1)\n",
    "dC_dw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b40f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.],\n",
       "        [0.],\n",
       "        [4.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manuelle Berechnung der partiellen Ableitung der Kostenfunktion nach den Gewichten W1\n",
    "dC_dw1_manual = c_1 * 2 * (da3_a0 -1)\n",
    "dC_dw1_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b44c5",
   "metadata": {},
   "source": [
    "### PINN mit Aktivierungsfunktion\n",
    "In untenstehendem Programmcode wird ein PINN mit der Kostenfunktion $C = (\\frac{dy}{dt} - 1)^2$ implementiert. Zunächst wird der Ausgabewert des Neuronalen Netzes $y$ berechnet. Dieser wird anschließend mithilfe des Backpropagation Algorithmus nach dem Eingabewert des Neuronalen Netzes $t$ abgeleitet. Die so berechnete Ableitung wird, wie zuvor beschrieben, in die Kostenfunktion integriert. Abschließend wird der Gradient dieser Kostenfunktion, die die Ableitung $\\frac{dy}{dt}$ enthält, erneut mittels Backpropagation ermittelt. Im Gegensatz zu obigem PINN wird hierbei die Sigmoid-Funktion als Aktivierungsfunktion eingesetzt. \n",
    "\n",
    "Eine Implementierung des Backpropagation-Algorithmus ohne Verwendung der pytorch Funktion autograd, ist für dieses PINN zu komplex. Daher ist der Backpropagation-Algorithmus für ein PINN mit Aktivierungsfunktion im nächsten Abschnitt anhand eines möglichst einfach strukturierten Neuronalen Netz veranschaulicht. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51fcc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Belegung der Gewichte, Bias und Eingangswert des NN\n",
    "a_0 = torch.tensor(2.0, requires_grad=True).view(-1, 1)\n",
    "\n",
    "W_1 = torch.tensor([1., -1., 2.], requires_grad=True).view(-1, 1)\n",
    "W_2 = torch.tensor([\n",
    "    [1., 1., 0.],\n",
    "    [0., -1., 2.],\n",
    "    [0., -1., 1]\n",
    "])\n",
    "W_3 = torch.tensor([1., 0., 1.], requires_grad=True).view(1, -1)\n",
    "\n",
    "b_1 = torch.tensor([0., -1., 1.]).view(-1, 1)\n",
    "b_2 = torch.tensor([-1., -1., 2.]).view(-1, 1)\n",
    "b_3 = torch.tensor([-1.]).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdc9b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung des Ausgabewertes des NN\n",
    "z_1 = torch.matmul(W_1, a_0) + b_1\n",
    "a_1 = sigmoid(z_1)\n",
    "z_2 = torch.matmul(W_2, a_1) + b_2\n",
    "a_2 = sigmoid(z_2)\n",
    "a_2.requires_grad_(True)\n",
    "z_3 = torch.matmul(W_3, a_2) + b_3\n",
    "a_3 = sigmoid(z_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9cbeb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0042]], grad_fn=<TBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Berechung der Ableitung der Ausgabe des NN nach dem Eingabewert\n",
    "da3_a0 = torch.autograd.grad(a_3, a_0, create_graph=True)[0]\n",
    "da3_a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cefe6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2387]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manuelle Berechnung der Ableitung dy/dt\n",
    "a_3 * (1 - a_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "357c2718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2387],\n",
       "        [0.0000],\n",
       "        [0.2387]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_0 = sigmoid(z_3) * (1 - sigmoid(z_3))\n",
    "c_0 = torch.matmul(W_3.t(), v_0)\n",
    "c_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "132c9374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2497],\n",
       "        [0.2021],\n",
       "        [0.0474]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_2 * (1 - a_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e515a9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0596],\n",
       "        [0.0483],\n",
       "        [0.0113]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_1 = c_0 * sigmoid(z_2) * (1 - sigmoid(z_2))\n",
    "c_1 = torch.matmul(W_2.t(), v_1)\n",
    "c_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ad68e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0042]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_2 = c_1 * sigmoid(z_1) * (1 - sigmoid(z_1))\n",
    "c_2 = torch.matmul(W_1.t(), v_2)\n",
    "c_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61404d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung der Kostenfunktion und der partiellen Ableitung der Kostenfunktion nach W1\n",
    "C = (da3_a0 - torch.tensor(1.0).view(-1, 1))**2\n",
    "dC_dw1 = torch.autograd.grad(C, W_1)\n",
    "dC_dw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d24b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Berechnungsgraph der partiellen Ableitungen nach a_0 und W1\n",
    "dot = make_dot(C, params={'a_0': a_0, 'W_1': W_1})\n",
    "dot.render(\"computation_graph_sigmoid_no_attr\", format=\"png\")  # Saves as computation_graph.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebafec43",
   "metadata": {},
   "source": [
    "### PINN mit Aktivierungsfunktion aus 3 Neuronen \n",
    "Nachfolgender Programmcode dient dazu die partiellen Ableitungen der Kostenfunktion nach den Gewichten $W_1$ und $W_2$ selbst herzuleiten und möglichst ohne die autograd Funktion von Pytorch zu berechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eb8455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Belegung der Variablen des Neuronalen Netzes\n",
    "a_0 = torch.tensor(2.0, requires_grad=True).view(1, -1)\n",
    "\n",
    "W_1 = torch.tensor(2.0, requires_grad=True).view(1, -1)\n",
    "W_2 = torch.tensor(-1.0, requires_grad=True).view(-1, 1)\n",
    "\n",
    "b_1 = torch.tensor(1).view(1, -1)\n",
    "b_2 = torch.tensor(2).view(1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5964e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung des Ausgabewertes des Neuronalen Netzes\n",
    "z_1 = torch.matmul(W_1, a_0) + b_1\n",
    "a_1 = sigmoid(z_1)\n",
    "z_2 = torch.matmul(W_2, a_1) + b_2\n",
    "a_2 = sigmoid(z_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "84fec73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0026]], grad_fn=<TBackward0>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Berechnung der Ableitung der Ausgabe des NN nach der ersten Eingabe (entspricht dy/dt in DGL)\n",
    "da2_a0 = torch.autograd.grad(a_2, a_0, create_graph=True, retain_graph=True)[0]\n",
    "da2_a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2246e569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0076]]),)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Berechung der Kostenfunktion und partiellen Ableitung nach W2 mithilfe von autograd\n",
    "y = da2_a0\n",
    "C = (y- torch.tensor(1.0).view(-1, 1))**2\n",
    "dC_dw2 = torch.autograd.grad(C, W_2, retain_graph=True)\n",
    "dC_dw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "142ad95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0076]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Berechung der partiellen Ableitung nach W2 vollständig ohne Hilfsmittel, nach der Produktregel\n",
    "c_1 = a_1 * (1- a_1)\n",
    "c2 = - (sigmoid(z_2) * (1 - sigmoid(z_2)))\n",
    "dc_dw2_fully_expanded = (2 * (y-1)) * ((((sigmoid(z_2) * (1- sigmoid(z_2)) * a_1 ) - 2 * a_2 * (sigmoid(z_2) * (1- sigmoid(z_2)) * a_1 )) * W_2) + (a_2 - a_2**2)) * (a_1 - a_1**2) * W_1\n",
    "dc_dw2_fully_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "43c398b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0076]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Berechung der partiellen Ableitung nach W2 unter Zuhilfenahme von bereits bei der Bestimmung dy/dt berechneten partiellen Ableitungen\n",
    "da2_dw2 = torch.autograd.grad(a_2, W_2, create_graph=True, retain_graph=True)[0]\n",
    "dC_dy = 2 * (y - 1)\n",
    "c_1 = (a_1 - a_1**2) * W_1\n",
    "c_2 = (a_2 - a_2**2) * W_2\n",
    "\n",
    "dC_dW2 = dC_dy * (((da2_dw2 - 2 * a_2 * da2_dw2) * W_2) + (a_2 - a_2**2)) * c_1\n",
    "dC_dW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e3c8e4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0077]]),)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Berechung der partiellen Ableitung nach W1 mithilfe von autograd\n",
    "dC_dw1 = torch.autograd.grad(C, W_1, retain_graph=True)\n",
    "dC_dw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "44700eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0077]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Berechung der partiellen Ableitung nach W2 unter Zuhilfenahme von bereits bei der Bestimmung dy/dt berechneten partiellen Ableitungen\n",
    "da2_dw1 = torch.autograd.grad(a_2, W_1, create_graph=True, retain_graph=True)[0]\n",
    "da1_dw1 = torch.autograd.grad(a_1, W_1, create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "dC_dy = 2 * (y - 1)\n",
    "c_1 = (a_1 - a_1**2) * W_1\n",
    "c_2 = (a_2 - a_2**2) * W_2\n",
    "\n",
    "dC_dW1 = dC_dy * ( ((da2_dw1 - 2 * a_2 * da2_dw1) * W_2 * c_1) + (((da1_dw1 - 2 * a_1 * da1_dw1) * W_1 * c_2) + (a_1 - a_1**2) * c_2))\n",
    "dC_dW1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
